Description: Use forw_bcast_list_lock always in disabled interrupt context
 forw_bcast_list_lock is spin_locked in both process and softirq
 context. SoftIRQ calls the spinlock with disabled IRQ and normal
 process context with enabled IRQs.
 .
 When process context is inside an spin_locked area protected by
 forw_bcast_list_lock and gets interrupted by an IRQ, it could happen
 that something tries to lock forw_bcast_list_lock again in SoftIRQ
 context. It cannot proceed further since the lock is already taken
 somewhere else, but no reschedule will happen inside the SoftIRQ
 context. This leads to an complete kernel hang without any chance of
 resurrection.
 .
 All functions called in process context must disable IRQs when they try
 to get get that lock to to prevent any reschedule due to IRQs.
Origin: backport, http://open-mesh.net/changeset/1504
Author: Sven Eckelmann <sven.eckelmann@gmx.de>

---
diff --git a/send.c b/send.c
index eb617508cca4c5e8481b8729f50f0b692dae2c71..979940ac9b00e4e1a0b6d56f0f0d55f1e30a3aa1 100644
--- a/send.c
+++ b/send.c
@@ -340,12 +340,13 @@ static void forw_packet_free(struct forw_packet *forw_packet)
 static void _add_bcast_packet_to_list(struct forw_packet *forw_packet,
 				      unsigned long send_time)
 {
+	unsigned long flags;
 	INIT_HLIST_NODE(&forw_packet->list);
 
 	/* add new packet to packet list */
-	spin_lock(&forw_bcast_list_lock);
+	spin_lock_irqsave(&forw_bcast_list_lock, flags);
 	hlist_add_head(&forw_packet->list, &forw_bcast_list);
-	spin_unlock(&forw_bcast_list_lock);
+	spin_unlock_irqrestore(&forw_bcast_list_lock, flags);
 
 	/* start timer for this packet */
 	INIT_DELAYED_WORK(&forw_packet->delayed_work,
@@ -384,10 +385,11 @@ void send_outstanding_bcast_packet(struct work_struct *work)
 		container_of(work, struct delayed_work, work);
 	struct forw_packet *forw_packet =
 		container_of(delayed_work, struct forw_packet, delayed_work);
+	unsigned long flags;
 
-	spin_lock(&forw_bcast_list_lock);
+	spin_lock_irqsave(&forw_bcast_list_lock, flags);
 	hlist_del(&forw_packet->list);
-	spin_unlock(&forw_bcast_list_lock);
+	spin_unlock_irqrestore(&forw_bcast_list_lock, flags);
 
 	/* rebroadcast packet */
 	rcu_read_lock();
@@ -438,24 +440,25 @@ void purge_outstanding_packets(void)
 {
 	struct forw_packet *forw_packet;
 	struct hlist_node *tmp_node, *safe_tmp_node;
+	unsigned long flags;
 
 	debug_log(LOG_TYPE_BATMAN, "purge_outstanding_packets()\n");
 
 	/* free bcast list */
-	spin_lock(&forw_bcast_list_lock);
+	spin_lock_irqsave(&forw_bcast_list_lock, flags);
 	hlist_for_each_entry_safe(forw_packet, tmp_node, safe_tmp_node,
 				  &forw_bcast_list, list) {
 
-		spin_unlock(&forw_bcast_list_lock);
+		spin_unlock_irqrestore(&forw_bcast_list_lock, flags);
 
 		/**
 		 * send_outstanding_bcast_packet() will lock the list to
 		 * delete the item from the list
 		 */
 		cancel_delayed_work_sync(&forw_packet->delayed_work);
-		spin_lock(&forw_bcast_list_lock);
+		spin_lock_irqsave(&forw_bcast_list_lock, flags);
 	}
-	spin_unlock(&forw_bcast_list_lock);
+	spin_unlock_irqrestore(&forw_bcast_list_lock, flags);
 
 	/* free batman packet list */
 	spin_lock(&forw_bat_list_lock);
